from transformers import AutoModelForCausalLM, AutoTokenizer
import torch, gc


SYSTEM_PROMPT = """
ä½ æ˜¯ä¸€åé¡¶çº§æ–‡ç”Ÿå›¾æç¤ºè¯ç”Ÿæˆä¸“å®¶ï¼ˆText-to-Image Prompt Engineerï¼‰ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æè¿°ï¼Œè½¬æ¢ä¸ºé«˜è´¨é‡ã€é€‚åˆ Stable Diffusion / ComfyUI / SDXL ç­‰æ¨¡å‹ä½¿ç”¨çš„æç¤ºè¯ã€‚ç”Ÿæˆçš„æç¤ºè¯åº”å…·å¤‡ä»¥ä¸‹ç‰¹ç‚¹ï¼š

1. **ç»†èŠ‚ä¸°å¯Œ**ï¼šæè¿°äººç‰©ã€åœºæ™¯ã€å…‰å½±ã€åŠ¨ä½œã€æè´¨ã€é¢œè‰²ã€æ°›å›´ç­‰ç»†èŠ‚ã€‚
2. **ç»“æ„åŒ–**ï¼šæŒ‰ [ä¸»ä½“]ã€[åŠ¨ä½œ/å§¿åŠ¿]ã€[æœé¥°/é“å…·]ã€[åœºæ™¯/ç¯å¢ƒ]ã€[å…‰å½±/æ°›å›´]ã€[é£æ ¼/è‰ºæœ¯å®¶å‚è€ƒ] è¿›è¡Œç»„ç»‡ã€‚
3. **é£æ ¼æŒ‡å‘æ˜ç¡®**ï¼šå¯åŒ…å«é£æ ¼ã€æ‘„å½±å‚æ•°ï¼ˆå¦‚ç„¦è·ã€é•œå¤´ã€æ™¯æ·±ï¼‰ã€ç”»è´¨è¦æ±‚ï¼ˆå¦‚4Kã€è¶…å†™å®ï¼‰ã€‚
4. **é¿å…æ¨¡ç³Šæè¿°**ï¼šä¸è¦ä½¿ç”¨â€œç¾ä¸½â€ã€â€œå¥½çœ‹â€ç­‰ä¸»è§‚è¯ï¼Œè€Œæ˜¯ç”¨å…·ä½“çš„è§†è§‰å…ƒç´ æè¿°ã€‚
5. **å¯é€‰æ‰©å±•**ï¼šå¯ç”Ÿæˆè´Ÿé¢æç¤ºè¯ï¼ˆNegative Promptï¼‰ï¼Œä»¥é¿å…ä¸å¸Œæœ›å‡ºç°çš„å…ƒç´ ã€‚

è¾“å‡ºåªè¿”å› JSONï¼Œä¸è¦æ·»åŠ è§£é‡Šæˆ–æ–‡æœ¬ã€‚  
JSON å­—æ®µä¸¥æ ¼éµå®ˆæ ¼å¼ã€‚

ç¤ºä¾‹è¾“å‡ºæ ¼å¼ï¼š
{
  "prompt": [ä¸»ä½“], [åŠ¨ä½œ/å§¿åŠ¿], [æœé¥°/é“å…·], [åœºæ™¯/ç¯å¢ƒ], [å…‰å½±/æ°›å›´], [é£æ ¼/è‰ºæœ¯å®¶å‚è€ƒ], [ç”»è´¨/æ‘„å½±å‚æ•°],
  "negative_prompt": [ä¸å¸Œæœ›å‡ºç°çš„å…ƒç´ ],
}
"""


class QwenLLM:
    def __init__(self):
        model_path = "./checkpoints/Qwen3-4B-Instruct"
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype="auto",
            device_map="auto",
        )

    def generate(self, prompt: str, max_new_tokens: int = 16384) -> str:
        messages = [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": prompt},
        ]
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
        )
        model_inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)
        generated_ids = self.model.generate(
            **model_inputs, max_new_tokens=max_new_tokens
        )
        output_ids = generated_ids[0][len(model_inputs.input_ids[0]) :].tolist()

        content = self.tokenizer.decode(output_ids, skip_special_tokens=True)
        return content

    def __del__(self):
        """ææ„æ—¶é‡Šæ”¾æ˜¾å­˜"""
        try:
            del self.tokenizer
            del self.model
        except AttributeError:
            pass
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()
        print("ğŸ§¹ QwenLLM destroyed, GPU memory released.")
